#%%
import re
from datetime import date, timedelta
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import quote
import time
import os
from datetime import timedelta
import json

HEADERS_BASE = {
    "User-Agent": "Mozilla/5.0"
}

def extract_oid_aid_key(url: str):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ URLì—ì„œ oidì™€ aidë¥¼ ì¶”ì¶œí•´ ê³ ìœ  ê¸°ì‚¬ key ìƒì„±"""
    m = re.search(r"/article/(\d+)/(\d+)", url)
    if not m:
        return None
    return f"{m.group(1)}_{m.group(2)}"


def is_financial_title(title: str, fin_keywords) -> bool:
    """ê¸°ì‚¬ ì œëª©ì— ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€ íŒë‹¨"""
    return any(k in title for k in fin_keywords)


def day_ranges(year: int):
    """í•´ë‹¹ ì—°ë„ì˜ ëª¨ë“  ë‚ ì§œë¥¼ í•˜ë£¨ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„± (ë¯¸ë˜ ë‚ ì§œ ì œì™¸)"""
    today = date.today()
    end = min(date(year, 12, 31), today)
    d = date(year, 1, 1)
    days = []
    while d <= end:
        days.append(d)
        d += timedelta(days=1)
    return days


def collect_links_day(keyword: str, day: date, headers, sleep_sec:float, fin_keywords=None):
    """íŠ¹ì • ë‚ ì§œì™€ í‚¤ì›Œë“œì— ëŒ€í•´ ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬ ëª©ë¡ ìˆ˜ì§‘"""
    q = quote(keyword)
    ds = day.strftime("%Y.%m.%d")

    url = (
        f"https://m.search.naver.com/search.naver"
        f"?where=m_news&query={q}&pd=3&ds={ds}&de={ds}"
    )

    res = requests.get(url, headers=headers, timeout=10)
    soup = BeautifulSoup(res.text, "html.parser")

    rows = []
    for a in soup.select("a[href*='n.news.naver.com/article']"):
        title = a.get("title") or a.text.strip()
        href = a.get("href", "")
        flag = is_financial_title(title, fin_keywords)

        key = extract_oid_aid_key(href)
        if not key:
            continue

        rows.append({
        "key": key,
        "keyword": keyword,
        "title": title,
        "url": href,
        "date": ds,
        "is_financial": int(flag)
        })

    time.sleep(sleep_sec)
    return rows

def parse_oid_aid(article_url):
    """ê¸°ì‚¬ URLì—ì„œ oid, aidë¥¼ ë¶„ë¦¬ ì¶”ì¶œ"""
    m = re.search(r"/article/(\d+)/(\d+)", article_url)
    if not m:
        return None, None
    return m.group(1), m.group(2)


def to_legacy_url(article_url):
    """ëŒ“ê¸€ API í˜¸ì¶œì„ ìœ„í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë ˆê±°ì‹œ URL ìƒì„±"""
    oid, aid = parse_oid_aid(article_url)
    if oid is None:
        return None
    return f"https://news.naver.com/main/read.nhn?oid={oid}&aid={aid}"

def safe_jsonp_load(text):
    """JSONP í˜•íƒœì˜ ë¬¸ìì—´ì„ ì•ˆì „í•˜ê²Œ JSONìœ¼ë¡œ íŒŒì‹±"""
    if "(" not in text or ")" not in text:
        return None
    try:
        return json.loads(text[text.find("(")+1 : text.rfind(")")])
    except:
        return None
    

def collect_comments(article_url, page_size, page_sleep):
    """ì»¤ì„œ ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜ì„ ì´ìš©í•´ ê¸°ì‚¬ ëŒ“ê¸€ ì „ì²´ ìˆ˜ì§‘"""
    legacy_url = to_legacy_url(article_url)
    if legacy_url is None:
        return []

    oid, aid = parse_oid_aid(article_url)
    object_id = f"news{oid},{aid}"

    headers = {
        **HEADERS_BASE,
        "Referer": legacy_url
    }

    base = (
        "https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json"
        "?ticket=news"
        "&templateId=view_politics"
        "&pool=cbox5"
        "&lang=ko"
        "&country=KR"
        f"&objectId={object_id.replace(',', '%2C')}"
        "&sort=favorite"
        "&initialize=true"
        f"&pageSize={page_size}"
    )

    all_comments = []
    seen_ids = set()

    next_cursor = None
    seen_cursors = set()

    while True:
        if next_cursor is None:
            url = base  # ì²« í˜ì´ì§€(ì´ˆê¸° ë¡œë”©)
        else:
            # ğŸ”¥ ë‹¤ìŒ í˜ì´ì§€ëŠ” page ë²ˆí˜¸ê°€ ì•„ë‹ˆë¼ cursorë¡œ ë„˜ê¹€
            url = (
                base
                + "&pageType=more"
                + f"&moreParam.next={next_cursor}"
                + "&initialize=false"
            )

        r = requests.get(url, headers=headers, timeout=10)
        data = safe_jsonp_load(r.text)
        if not data:
            break

        result = data.get("result", {})
        comment_list = result.get("commentList", [])
        if not comment_list:
            break

        new_count = 0
        for c in comment_list:
            cid = c.get("commentNo")
            if cid is None or cid in seen_ids:
                continue
            seen_ids.add(cid)
            new_count += 1

            all_comments.append({
                "comment_id": cid,
                "article_url": article_url,
                "contents": c.get("contents", "").replace("\n", " ").strip(),
                "sympathy": c.get("sympathyCount", 0),
                "antipathy": c.get("antipathyCount", 0),
                "reg_time": c.get("regTime")
            })

        # ìƒˆ ëŒ“ê¸€ì´ ë” ì´ìƒ ì•ˆ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
        if new_count == 0:
            break

        mp = result.get("morePage", {})
        next_cursor_new = mp.get("next")

        # next ì»¤ì„œê°€ ì—†ê±°ë‚˜, ë°˜ë³µë˜ë©´ ì¢…ë£Œ(ë¬´í•œë£¨í”„ ë°©ì§€)
        if not next_cursor_new or next_cursor_new in seen_cursors:
            break

        seen_cursors.add(next_cursor_new)
        next_cursor = next_cursor_new

        time.sleep(page_sleep)

    return all_comments