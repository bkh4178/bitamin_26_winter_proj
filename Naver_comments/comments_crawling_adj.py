#%%
import requests
import json
import pandas as pd
import time
import re
import os

# --------------------------------------------------
# ì„¤ì •
# --------------------------------------------------
HEADERS_BASE = {
    "User-Agent": "Mozilla/5.0"
}

BASE_LIST_URL = (
    "https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json"
    "?ticket=news"
    "&templateId=view_politics"
    "&pool=cbox5"
    "&lang=ko"
    "&country=KR"
    "&objectId={object_id}"
    "&pageSize={page_size}"
    "&page={page}"
    "&sort=favorite"
    "&initialize=true"
)

ARTICLE_CSV = "../data/NAVER/article/articles_2025_financial.csv"
OUTPUT_CSV = "../data/NAVER/comments/comments_2025_adj.csv"

PAGE_SIZE = 100      # ë„¤ì´ë²„ ì„œë²„ê°€ ì‚¬ì‹¤ìƒ í—ˆìš©í•˜ëŠ” ìµœëŒ€
MAX_PAGES = 100      # ì•ˆì „ ì¥ì¹˜
ARTICLE_SLEEP = 0.5
PAGE_SLEEP = 0.2

# --------------------------------------------------
# oid / aid ì¶”ì¶œ
# --------------------------------------------------
def parse_oid_aid(article_url):
    m = re.search(r"/article/(\d+)/(\d+)", article_url)
    if not m:
        return None, None
    return m.group(1), m.group(2)

def to_legacy_url(article_url):
    oid, aid = parse_oid_aid(article_url)
    if oid is None:
        return None
    return f"https://news.naver.com/main/read.nhn?oid={oid}&aid={aid}"

# --------------------------------------------------
# JSONP ì•ˆì „ íŒŒì„œ
# --------------------------------------------------
def safe_jsonp_load(text):
    if "(" not in text or ")" not in text:
        return None
    try:
        return json.loads(text[text.find("(")+1 : text.rfind(")")])
    except:
        return None

# --------------------------------------------------
# ëŒ“ê¸€ ì „ì²´ ìˆ˜ì§‘ (ì¤‘ë³µ ID ê°ì§€ ì¢…ë£Œ)
# --------------------------------------------------
def collect_comments(article_url):
    legacy_url = to_legacy_url(article_url)
    if legacy_url is None:
        return []

    oid, aid = parse_oid_aid(article_url)
    object_id = f"news{oid},{aid}"

    headers = {
        **HEADERS_BASE,
        "Referer": legacy_url
    }

    base = (
        "https://apis.naver.com/commentBox/cbox/web_naver_list_jsonp.json"
        "?ticket=news"
        "&templateId=view_politics"
        "&pool=cbox5"
        "&lang=ko"
        "&country=KR"
        f"&objectId={object_id.replace(',', '%2C')}"
        "&sort=favorite"
        "&initialize=true"
        f"&pageSize={PAGE_SIZE}"
    )

    all_comments = []
    seen_ids = set()

    next_cursor = None
    seen_cursors = set()

    while True:
        if next_cursor is None:
            url = base  # ì²« í˜ì´ì§€(ì´ˆê¸° ë¡œë”©)
        else:
            # ğŸ”¥ ë‹¤ìŒ í˜ì´ì§€ëŠ” page ë²ˆí˜¸ê°€ ì•„ë‹ˆë¼ cursorë¡œ ë„˜ê¹€
            url = (
                base
                + "&pageType=more"
                + f"&moreParam.next={next_cursor}"
                + "&initialize=false"
            )

        r = requests.get(url, headers=headers, timeout=10)
        data = safe_jsonp_load(r.text)
        if not data:
            break

        result = data.get("result", {})
        comment_list = result.get("commentList", [])
        if not comment_list:
            break

        new_count = 0
        for c in comment_list:
            cid = c.get("commentNo")
            if cid is None or cid in seen_ids:
                continue
            seen_ids.add(cid)
            new_count += 1

            all_comments.append({
                "comment_id": cid,
                "article_url": article_url,
                "contents": c.get("contents", "").replace("\n", " ").strip(),
                "sympathy": c.get("sympathyCount", 0),
                "antipathy": c.get("antipathyCount", 0),
                "reg_time": c.get("regTime")
            })

        # ìƒˆ ëŒ“ê¸€ì´ ë” ì´ìƒ ì•ˆ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
        if new_count == 0:
            break

        mp = result.get("morePage", {})
        next_cursor_new = mp.get("next")

        # next ì»¤ì„œê°€ ì—†ê±°ë‚˜, ë°˜ë³µë˜ë©´ ì¢…ë£Œ(ë¬´í•œë£¨í”„ ë°©ì§€)
        if not next_cursor_new or next_cursor_new in seen_cursors:
            break

        seen_cursors.add(next_cursor_new)
        next_cursor = next_cursor_new

        time.sleep(PAGE_SLEEP)

    return all_comments

# --------------------------------------------------
# ë©”ì¸ ì‹¤í–‰ (ê¸°ì‚¬ ë‹¨ìœ„ append ì €ì¥)
# --------------------------------------------------
def main():
    os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)

    df = pd.read_csv(ARTICLE_CSV)
    first_write = not os.path.exists(OUTPUT_CSV)

    print("ì´ ê¸°ì‚¬ ìˆ˜:", len(df))

    for i, row in df.iterrows():
        article_url = row["url"]
        print(f"[{i+1}/{len(df)}] ëŒ“ê¸€ ìˆ˜ì§‘:", article_url)

        comments = collect_comments(article_url)
        print("  ìˆ˜ì§‘ ëŒ“ê¸€ ìˆ˜:", len(comments))

        if comments:
            out_df = pd.DataFrame(comments)
            out_df.to_csv(
                OUTPUT_CSV,
                mode="a",
                header=first_write,
                index=False,
                encoding="utf-8-sig"
            )
            first_write = False

        time.sleep(ARTICLE_SLEEP)

    print("\nâœ… ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
    print("ì €ì¥ íŒŒì¼:", OUTPUT_CSV)

if __name__ == "__main__":
    main()
#%%
comments = collect_comments("https://n.news.naver.com/article/011/0004445148")
print("ìˆ˜ì§‘ ëŒ“ê¸€ ìˆ˜:", len(comments))