#%%
# article_crawling.py
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import quote
import time
import os
from datetime import date, timedelta

from utils import extract_oid_aid_key, is_financial_title

# -----------------------------
# ì„¤ì •
# -----------------------------
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) "
        "AppleWebKit/605.1.15 (KHTML, like Gecko) "
        "Version/16.0 Mobile/15E148 Safari/604.1"
    )
}

KEYWORDS = ["í­ë½", "ê¸‰ë½", "ê¸‰ë“±", "ë°˜ë“±", "ì¡°ì •", "ê³¼ì—´", "ë²„ë¸”", "íŒ¨ë‹‰", "ë ë¦¬"]
YEAR = 2025
SLEEP_SEC = 0.3

OUTPUT_DIR = "../data/NAVER/article"
OUTPUT_PATH = f"{OUTPUT_DIR}/articles_2025_financial.csv"

# -----------------------------
# ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ (ë¯¸ë˜ ë‚ ì§œ ì œì™¸)
# -----------------------------
def day_ranges(year: int):
    today = date.today()
    end = min(date(year, 12, 31), today)
    d = date(year, 1, 1)
    days = []
    while d <= end:
        days.append(d)
        d += timedelta(days=1)
    return days

# -----------------------------
# ì¼ Ã— í‚¤ì›Œë“œ ê¸°ì‚¬ ìˆ˜ì§‘
# -----------------------------
def collect_links_day(keyword: str, day: date):
    q = quote(keyword)
    ds = day.strftime("%Y.%m.%d")

    url = (
        f"https://m.search.naver.com/search.naver"
        f"?where=m_news&query={q}&pd=3&ds={ds}&de={ds}"
    )

    res = requests.get(url, headers=HEADERS, timeout=10)
    soup = BeautifulSoup(res.text, "html.parser")

    rows = []
    for a in soup.select("a[href*='n.news.naver.com/article']"):
        title = a.get("title") or a.text.strip()
        href = a.get("href", "")
        flag = is_financial_title(title)

        key = extract_oid_aid_key(href)
        if not key:
            continue

        rows.append({
        "key": key,
        "keyword": keyword,
        "title": title,
        "url": href,
        "date": ds,
        "is_financial": int(flag)
        })

        key = extract_oid_aid_key(href)
        if not key:
            continue
        rows.append({
            "key": key,
            "keyword": keyword,
            "title": title,
            "url": href,
            "date": ds
        })

    time.sleep(SLEEP_SEC)
    return rows

# -----------------------------
# ë©”ì¸
# -----------------------------
def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # key(oid+aid) ê¸°ì¤€ìœ¼ë¡œë§Œ ì¤‘ë³µ ì œê±°
    uniq = {}

    days = day_ranges(YEAR)
    print(f"ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ ìˆ˜: {len(days)}")

    for d in days:
        print(f"\nğŸ“… {d}")
        for kw in KEYWORDS:
            rows = collect_links_day(kw, d)
            for r in rows:
                uniq.setdefault(r["key"], r)

    df = pd.DataFrame(uniq.values()).drop_duplicates(subset=["url"])
    df.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")

    print("\nâœ… ì™„ë£Œ")
    print("ì´ ê¸°ì‚¬ ìˆ˜:", len(df))
    print("ì €ì¥ ìœ„ì¹˜:", OUTPUT_PATH)

if __name__ == "__main__":
    main()

#%%
import pandas as pd

df = pd.read_csv("../data/NAVER/article/articles_2025_financial.csv")

print("ì´ ê¸°ì‚¬ ìˆ˜:", len(df))
print("is_financial=1 ë¹„ìœ¨:", df["is_financial"].mean())

daily = df.groupby("date").size()
print("0ê±´ì¸ ë‚ ì§œ ìˆ˜:", (daily==0).sum())  # ì›ë˜ ê±°ì˜ 0ì´ ëœ° ê±°ë¼ ì˜ë¯¸ ì—†ìŒ
print("1ê°œ ë¯¸ë§Œ(=0) ë‚ ì§œ ìˆ˜:", (daily<1).sum())
print("í•˜ë£¨ í‰ê· :", daily.mean())
print("í•˜ë£¨ ì¤‘ì•™ê°’:", daily.median())